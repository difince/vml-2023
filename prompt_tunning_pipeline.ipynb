{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a43166e-3c50-4a2d-886f-a978c65c73c4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Kubeflow and Prompt Tunning\n",
    "\n",
    "This notebook trains a foundational LLM by prompt tuning to perform better in classifying tweets as complaints or non-complaints, publishes the trained configuration to HuggingFace, and serves the model.\n",
    "Kubeflow Pipelines automate and orchestrate the entire process.\n",
    "\n",
    "#### What's used?\n",
    "\n",
    "1. [bigscience/bloomz-560m](https://huggingface.co/bigscience/bloomz-560m#model-summary) - a foundational model prompt tunned in this notebook\n",
    "2. [PEFT](https://huggingface.co/docs/peft/index) - Parameter-Efficient Fine-Tuning (PEFT)- a ðŸ¤— HuggingFace liberary used for tine tuning/prompt tuning opensource LLM \n",
    "3. [RAFT](https://huggingface.co/datasets/ought/raft) dataset - Real-world Annotated Few-shot Tasks (RAFT) dataset - an aggregation of English-language datasets found in the real world. We are using `twitter compains` subset specified in classifing sentiment of tweets (`compain` or `not complain`)  \n",
    "\n",
    "\n",
    "#### GitHub Repository\n",
    "To test this example on your own, you will need to follow the setup instructions described here: \n",
    "- [vml-2023](https://github.com/difince/vml-2023) - a github reporitory which contains the code of this notebook and setup instructions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f5fd16-9833-42b9-b062-a31b22f96d60",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Run through the Notebook\n",
    "\n",
    "After installing kfp, you must restart the kernel for the new version to take affect. After the restart, run through all cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c40ced32-fb30-4ad8-821b-e2c4d32ec90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kfp==2.3.0 in /opt/conda/lib/python3.8/site-packages (2.3.0)\n",
      "Requirement already satisfied: click<9,>=8.0.0 in /opt/conda/lib/python3.8/site-packages (from kfp==2.3.0) (8.1.7)\n",
      "Requirement already satisfied: docstring-parser<1,>=0.7.3 in /opt/conda/lib/python3.8/site-packages (from kfp==2.3.0) (0.15)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /opt/conda/lib/python3.8/site-packages (from kfp==2.3.0) (2.10.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.1 in /opt/conda/lib/python3.8/site-packages (from kfp==2.3.0) (2.23.4)\n",
      "Requirement already satisfied: google-cloud-storage<3,>=2.2.1 in /opt/conda/lib/python3.8/site-packages (from kfp==2.3.0) (2.13.0)\n",
      "Requirement already satisfied: kfp-pipeline-spec==0.2.2 in /opt/conda/lib/python3.8/site-packages (from kfp==2.3.0) (0.2.2)\n",
      "Requirement already satisfied: kfp-server-api<2.1.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from kfp==2.3.0) (2.0.3)\n",
      "Requirement already satisfied: kubernetes<27,>=8.0.0 in /opt/conda/lib/python3.8/site-packages (from kfp==2.3.0) (12.0.1)\n",
      "Requirement already satisfied: protobuf<4,>=3.13.0 in /opt/conda/lib/python3.8/site-packages (from kfp==2.3.0) (3.19.6)\n",
      "Requirement already satisfied: PyYAML<7,>=5.3 in /opt/conda/lib/python3.8/site-packages (from kfp==2.3.0) (5.4.1)\n",
      "Requirement already satisfied: requests-toolbelt<1,>=0.8.0 in /opt/conda/lib/python3.8/site-packages (from kfp==2.3.0) (0.10.1)\n",
      "Requirement already satisfied: tabulate<1,>=0.8.6 in /opt/conda/lib/python3.8/site-packages (from kfp==2.3.0) (0.9.0)\n",
      "Requirement already satisfied: urllib3<2.0.0 in /opt/conda/lib/python3.8/site-packages (from kfp==2.3.0) (1.26.16)\n",
      "Requirement already satisfied: typing-extensions<5,>=3.7.4 in /opt/conda/lib/python3.8/site-packages (from kfp==2.3.0) (4.7.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/lib/python3.8/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp==2.3.0) (1.60.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.8/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp==2.3.0) (2.31.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.1->kfp==2.3.0) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.1->kfp==2.3.0) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.1->kfp==2.3.0) (4.9)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /opt/conda/lib/python3.8/site-packages (from google-cloud-storage<3,>=2.2.1->kfp==2.3.0) (2.3.3)\n",
      "Requirement already satisfied: google-resumable-media>=2.6.0 in /opt/conda/lib/python3.8/site-packages (from google-cloud-storage<3,>=2.2.1->kfp==2.3.0) (2.6.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.8/site-packages (from google-cloud-storage<3,>=2.2.1->kfp==2.3.0) (1.5.0)\n",
      "Requirement already satisfied: six>=1.10 in /opt/conda/lib/python3.8/site-packages (from kfp-server-api<2.1.0,>=2.0.0->kfp==2.3.0) (1.16.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.8/site-packages (from kfp-server-api<2.1.0,>=2.0.0->kfp==2.3.0) (2023.7.22)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.8/site-packages (from kfp-server-api<2.1.0,>=2.0.0->kfp==2.3.0) (2.8.2)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /opt/conda/lib/python3.8/site-packages (from kubernetes<27,>=8.0.0->kfp==2.3.0) (68.1.2)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.8/site-packages (from kubernetes<27,>=8.0.0->kfp==2.3.0) (1.6.2)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.8/site-packages (from kubernetes<27,>=8.0.0->kfp==2.3.0) (1.3.1)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.1->kfp==2.3.0) (0.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.8/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp==2.3.0) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp==2.3.0) (3.4)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.8/site-packages (from requests-oauthlib->kubernetes<27,>=8.0.0->kfp==2.3.0) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install kfp==2.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4f783ca-3942-4884-9791-2faa4c001326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "from kfp.dsl import component\n",
    "import kfp.components as comp\n",
    "\n",
    "print(kfp.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84402db0-d5e9-405c-9fcc-3087e7338f9e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Kubeflow Pipeline\n",
    "A Kubeflow pipeline (KFP) is used for building and deploying portable and scalable machine learning (ML) workflows, based on `Docker containers`. A pipeline is composed of a set of input parameters and a list of the steps in this workflow. Each step in a pipeline is an instance of a component.\n",
    "\n",
    "<!-- With KFP you can \n",
    "- author components and pipelines using the `KFP Python SDK`, \n",
    "- compile pipelines to an intermediate representation YAML, \n",
    "- submit the pipeline to KFP-conformant backend for execution. -->\n",
    "\n",
    "The `dsl.component` and `dsl.pipeline` decorators turn your type-annotated Python functions into components and pipelines, respectively. \n",
    "\n",
    "Each Pipeline component is executed in a separate container. For each we need to define the `packages_to_install` and the `base_image`\n",
    "<!-- The PromptTuningConfig contains information about the task type, the text to initialize the prompt embedding, the number of virtual tokens, and the tokenizer to use -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5c724f-10e9-4bc0-b2ce-8aba440c76fb",
   "metadata": {},
   "source": [
    "### Read huggingface-secret \n",
    "\n",
    "One of the setup instructions steps is to create a Kubernetes secret that contains our HuggingFace Write token, as the pipeline is going to push the trained configuration to HuggingFace. The secret can be generated with the following command: \n",
    "\n",
    "`kubectl create secret generic huggingface-secret --from-literal='token=<HuggingFace_WRITE_Token>' -n kubeflow`\n",
    "\n",
    "Our pipeline's initial step, <unk> get_hf_token<unk>, retrieves our HuggintFace token from the secret we had previously created. The token is passed as an input parameter to the next pipeline step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43365c0f-524e-407c-ac4d-2e757ec8ceb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"kubernetes\"],\n",
    "    base_image='python:3.10'\n",
    ")\n",
    "def get_hf_token() -> str:\n",
    "    from kubernetes import client, config\n",
    "\n",
    "    config.load_incluster_config()\n",
    "    core_api = client.CoreV1Api()\n",
    "    secret = core_api.read_namespaced_secret(name=\"huggingface-secret\", namespace=\"kubeflow-user-example-com\")\n",
    "    return secret.data[\"token\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32314ed1-199d-4a22-bd84-e9e6384c7b8b",
   "metadata": {},
   "source": [
    "### Prompt Tuning \n",
    "\"prompt_tuning_bloom\" pipeline step defines the model and tokenizer, the dataset and the dataset columns to train on, some training hyperparameters, and the PromptTuningConfig. The PromptTuningConfig contains information about the task type, the text to initialize the prompt embedding, the number of virtual tokens, and the tokenizer to use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4ffdbf8-d014-4132-8b83-503c05125af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"peft\", \"transformers==4.34.0\", \"datasets\", \"torch\", \"datasets\", \"tqdm\"],\n",
    "    base_image='python:3.10'\n",
    ")\n",
    "def prompt_tuning_bloom(huggingface_name: str, peft_model_publish_id: str, model_name_or_path: str, num_epochs: int, hf_token: str):\n",
    "    import transformers\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer, default_data_collator, get_linear_schedule_with_warmup\n",
    "    from peft import get_peft_config, get_peft_model, PromptTuningInit, PromptTuningConfig, TaskType, PeftType\n",
    "    import torch\n",
    "    from datasets import load_dataset\n",
    "    import os\n",
    "    from torch.utils.data import DataLoader\n",
    "    from tqdm import tqdm\n",
    "    import base64\n",
    "\n",
    "    peft_config = PromptTuningConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        prompt_tuning_init=PromptTuningInit.TEXT,\n",
    "        num_virtual_tokens=8,\n",
    "        prompt_tuning_init_text=\"Classify if the tweet is a complaint or not:\",\n",
    "        tokenizer_name_or_path=model_name_or_path, # bigscience/bloomz-560m\n",
    "    )\n",
    "\n",
    "    dataset_name = \"twitter_complaints\"\n",
    "    text_column = \"Tweet text\"\n",
    "    label_column = \"text_label\"\n",
    "    max_length = 64\n",
    "    lr = 3e-2\n",
    "    batch_size = 8\n",
    "\n",
    "    # Load the twitter_complaints subset of the RAFT dataset. This subset contains tweets that are labeled either complaint or no complaint\n",
    "    dataset = load_dataset(\"ought/raft\", dataset_name)\n",
    "    dataset[\"train\"][0]\n",
    "\n",
    "    # Make the Label column more readable -> replace the Label value with the corresponding label text and store them in a `text_label` column. The `map` function makes it possible to apply this change over the entire dataset in one step.\n",
    "    classes = [k.replace(\"_\", \" \") for k in dataset[\"train\"].features[\"Label\"].names]\n",
    "    dataset = dataset.map(\n",
    "        lambda x: {\"text_label\": [classes[label] for label in x[\"Label\"]]},\n",
    "        batched=True,\n",
    "        num_proc=1,\n",
    "    )\n",
    "\n",
    "    # Preprocess dataset - by setting tokenizer.\n",
    "    # Configure the appropriate padding token to use for padding sequences, and determine the maximum length of the tokenized labels\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "        \n",
    "    # Create a preprocess_function to:\n",
    "    # Tokenize the input text and labels.\n",
    "    # For each example in a batch, pad the labels with the tokenizers pad_token_id.\n",
    "    # Concatenate the input text and labels into the model_inputs.\n",
    "    # Create a separate attention mask for labels and model_inputs.\n",
    "    # Loop through each example in the batch again to pad the input ids, labels, and attention mask to the max_length and convert them to PyTorch tensors.\n",
    "    def preprocess_function(examples):\n",
    "        batch_size = len(examples[text_column])\n",
    "        inputs = [f\"{text_column} : {x} Label : \" for x in examples[text_column]]\n",
    "        targets = [str(x) for x in examples[label_column]]\n",
    "        model_inputs = tokenizer(inputs)\n",
    "        labels = tokenizer(targets)\n",
    "        for i in range(batch_size):\n",
    "            sample_input_ids = model_inputs[\"input_ids\"][i]\n",
    "            label_input_ids = labels[\"input_ids\"][i] + [tokenizer.pad_token_id]\n",
    "            model_inputs[\"input_ids\"][i] = sample_input_ids + label_input_ids\n",
    "            labels[\"input_ids\"][i] = [-100] * len(sample_input_ids) + label_input_ids\n",
    "            model_inputs[\"attention_mask\"][i] = [1] * len(model_inputs[\"input_ids\"][i])\n",
    "        for i in range(batch_size):\n",
    "            sample_input_ids = model_inputs[\"input_ids\"][i]\n",
    "            label_input_ids = labels[\"input_ids\"][i]\n",
    "            model_inputs[\"input_ids\"][i] = [tokenizer.pad_token_id] * (\n",
    "                max_length - len(sample_input_ids)\n",
    "            ) + sample_input_ids\n",
    "            model_inputs[\"attention_mask\"][i] = [0] * (max_length - len(sample_input_ids)) + model_inputs[\n",
    "                \"attention_mask\"\n",
    "            ][i]\n",
    "            labels[\"input_ids\"][i] = [-100] * (max_length - len(sample_input_ids)) + label_input_ids\n",
    "            model_inputs[\"input_ids\"][i] = torch.tensor(model_inputs[\"input_ids\"][i][:max_length])\n",
    "            model_inputs[\"attention_mask\"][i] = torch.tensor(model_inputs[\"attention_mask\"][i][:max_length])\n",
    "            labels[\"input_ids\"][i] = torch.tensor(labels[\"input_ids\"][i][:max_length])\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        return model_inputs\n",
    "    \n",
    "    # Use the map function to apply the preprocess_function to the entire dataset. You can remove the unprocessed columns since the model wonâ€™t need them:\n",
    "    processed_datasets = dataset.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        num_proc=1,\n",
    "        remove_columns=dataset[\"train\"].column_names,\n",
    "        load_from_cache_file=False,\n",
    "        desc=\"Running tokenizer on dataset\",\n",
    "    )\n",
    "\n",
    "    # Create a DataLoader from the train and eval datasets. Set pin_memory=True to speed up the data transfer to the GPU during training if the samples in your dataset are on a CPU.\n",
    "    train_dataset = processed_datasets[\"train\"]\n",
    "    eval_dataset = processed_datasets[\"train\"]\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, shuffle=True, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=False\n",
    "    )\n",
    "    eval_dataloader = DataLoader(eval_dataset, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=False)\n",
    "    \n",
    "    # Initialize a base model from AutoModelForCausalLM, and pass it and peft_config to the get_peft_model() function to create a PeftModel. \n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name_or_path) #  bigscience/bloomz-560m\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    \n",
    "    # Print the new PeftModelâ€™s trainable parameters to see how much more efficient it is than training the full parameters of the original model!\n",
    "    model.print_trainable_parameters()\n",
    "    \n",
    "    # Setup an optimizer and learning rate scheduler:\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    lr_scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=(len(train_dataloader) * num_epochs),\n",
    "    )\n",
    "\n",
    "    # Define training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "            batch = {k: v for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.detach().float()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        model.eval()\n",
    "        eval_loss = 0\n",
    "        eval_preds = []\n",
    "        for step, batch in enumerate(tqdm(eval_dataloader)):\n",
    "            batch = {k: v for k, v in batch.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            eval_loss += loss.detach().float()\n",
    "            eval_preds.extend(\n",
    "                tokenizer.batch_decode(torch.argmax(outputs.logits, -1).detach().cpu().numpy(), skip_special_tokens=True)\n",
    "            )\n",
    "\n",
    "        eval_epoch_loss = eval_loss / len(eval_dataloader)\n",
    "        eval_ppl = torch.exp(eval_epoch_loss)\n",
    "        train_epoch_loss = total_loss / len(train_dataloader)\n",
    "        train_ppl = torch.exp(train_epoch_loss)\n",
    "        print(\"epoch=%s: train_ppl=%s train_epoch_loss=%s eval_ppl=%s eval_epoch_loss=%s\" % (epoch, train_ppl, train_epoch_loss, eval_ppl, eval_epoch_loss))\n",
    "\n",
    "    # Store and share your model \n",
    "    from huggingface_hub import login\n",
    "    login(token=base64.b64decode(hf_token).decode())\n",
    "\n",
    "    peft_model_id = \"{}/{}\".format(huggingface_name, peft_model_publish_id)\n",
    "    model.save_pretrained(\"output_dir\", safe_serialization=False)\n",
    "    \n",
    "    # Use the push_to_hub function to upload your model to a model repository on the Hub:\n",
    "    model.push_to_hub(peft_model_id, use_auth_token=True, safe_serialization=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e112dd14-d570-49d8-8416-8657d745dc1b",
   "metadata": {},
   "source": [
    "### MLServe & Building an image\n",
    "\n",
    "Usually, we deploy models as microservices with a few exposed endpoints. To do so, we use an open sourced Python library called MLServe that wraps our model with endpoints and build microservices. And as we work on top of Kubernetes, we need the model class and its dependencies packaged within an image. The image has already been created and published, allowing us to use it right away:  `quay.io/aipipeline/peft-model-server:latest` . If you would like to see how it is created, you may take a look at [peft_model_server.py](https://github.com/kubeflow/kfp-tekton/blob/master/samples/peft-modelmesh-pipeline/peft_model_server.py) and the [Dockerfile](https://github.com/kubeflow/kfp-tekton/blob/master/samples/peft-modelmesh-pipeline/Dockerfile) .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefa2d00-fff9-49cf-a6d0-608b77604d5f",
   "metadata": {},
   "source": [
    "### Deploy Serving Runtime\n",
    "\n",
    "#### ServingRuntime\n",
    "A ServingRuntime defines the templates for Pods that can serve one or more particular model formats. \n",
    "KServe and ModelMesh installations come with a few out-of-the-box ServingRuntimes, but we are going to extend this existing list and create a Custom Serving Runtime.\n",
    " \n",
    "Once this step is executed you could validate it with the following command: \n",
    "```bash \n",
    "kubectl get  servingruntime -n modelmesh-serving\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecfb65cc-2841-468c-80a3-18cc9f7f257f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"kubernetes\"],\n",
    "    base_image='python:3.10'\n",
    ")\n",
    "def deploy_modelmesh_custom_runtime(huggingface_name: str, peft_model_publish_id: str, model_name_or_path: str, server_name: str, namespace: str, image: str):\n",
    "    import kubernetes.config as k8s_config\n",
    "    import kubernetes.client as k8s_client\n",
    "    from kubernetes.client.exceptions import ApiException\n",
    "\n",
    "    def create_custom_object(group, version, namespace, plural, manifest):\n",
    "        cfg = k8s_client.Configuration()\n",
    "        cfg.verify_ssl=False\n",
    "        cfg.host = \"https://kubernetes.default.svc\"\n",
    "        cfg.api_key_prefix['authorization'] = 'Bearer'\n",
    "        cfg.ssl_ca_cert = '/var/run/secrets/kubernetes.io/serviceaccount/ca.crt'\n",
    "        with open(\"/var/run/secrets/kubernetes.io/serviceaccount/token\") as f:\n",
    "            lines = f.readlines()\n",
    "            for l in lines:\n",
    "                cfg.api_key['authorization'] = \"{}\".format(l)\n",
    "                break\n",
    "        with k8s_client.ApiClient(cfg) as api_client:\n",
    "            capi = k8s_client.CustomObjectsApi(api_client)\n",
    "            try:\n",
    "                res = capi.create_namespaced_custom_object(group=group,\n",
    "                                                           version=version, namespace=namespace,\n",
    "                                                           plural=plural, body=manifest)\n",
    "            except ApiException as e:\n",
    "                # object already exists\n",
    "                if e.status != 409:\n",
    "                    raise\n",
    "    custom_runtime_manifest = {\n",
    "        \"apiVersion\": \"serving.kserve.io/v1alpha1\",\n",
    "        \"kind\": \"ServingRuntime\",\n",
    "        \"metadata\": {\n",
    "            \"name\": \"{}-server\".format(server_name), #vml-demo-server\n",
    "            \"namespace\": namespace\n",
    "        },\n",
    "        \"spec\": {\n",
    "            \"supportedModelFormats\": [\n",
    "            {\n",
    "                \"name\": \"peft-model\",\n",
    "                \"version\": \"1\",\n",
    "                \"autoSelect\": True\n",
    "            }\n",
    "            ],\n",
    "            \"multiModel\": True,\n",
    "            \"grpcDataEndpoint\": \"port:8001\",\n",
    "            \"grpcEndpoint\": \"port:8085\",\n",
    "            \"replicas\": 1,\n",
    "            \"containers\": [\n",
    "            {\n",
    "                \"name\": \"mlserver\",\n",
    "                \"image\": image, # quay.io/aipipeline/peft-model-server:latest\n",
    "                \"env\": [\n",
    "                {\n",
    "                    \"name\": \"MLSERVER_MODELS_DIR\",\n",
    "                    \"value\": \"/models/_mlserver_models/\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"MLSERVER_GRPC_PORT\",\n",
    "                    \"value\": \"8001\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"MLSERVER_HTTP_PORT\",\n",
    "                    \"value\": \"8002\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"MLSERVER_LOAD_MODELS_AT_STARTUP\",\n",
    "                    \"value\": \"true\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"MLSERVER_MODEL_NAME\",\n",
    "                    \"value\": \"peft-model\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"MLSERVER_HOST\",\n",
    "                    \"value\": \"127.0.0.1\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"MLSERVER_GRPC_MAX_MESSAGE_LENGTH\",\n",
    "                    \"value\": \"-1\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"PRETRAINED_MODEL_PATH\",\n",
    "                    \"value\": model_name_or_path\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"PEFT_MODEL_ID\",\n",
    "                    \"value\": \"{}/{}\".format(huggingface_name, peft_model_publish_id),\n",
    "                }\n",
    "                ],\n",
    "                \"resources\": {\n",
    "                \"requests\": {\n",
    "                    \"cpu\": \"500m\",\n",
    "                    \"memory\": \"4Gi\"\n",
    "                },\n",
    "                \"limits\": {\n",
    "                    \"cpu\": \"5\",\n",
    "                    \"memory\": \"5Gi\"\n",
    "                }\n",
    "                }\n",
    "            }\n",
    "            ],\n",
    "            \"builtInAdapter\": {\n",
    "            \"serverType\": \"mlserver\",\n",
    "            \"runtimeManagementPort\": 8001,\n",
    "            \"memBufferBytes\": 134217728,\n",
    "            \"modelLoadingTimeoutMillis\": 90000\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    create_custom_object(group=\"serving.kserve.io\", version=\"v1alpha1\",\n",
    "                         namespace=namespace, plural=\"servingruntimes\",\n",
    "                         manifest=custom_runtime_manifest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406adf5d-6e75-43cd-bb9d-ad2488037a71",
   "metadata": {},
   "source": [
    "### Deploy the Model\n",
    "\n",
    "To deploy the model, we create InferenceService. It states where the model reside, it specify the model fomats it can serve (peft-model). In addition, we also specify the serving runtime that should be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2e273e4-270c-40a9-bd0b-0061983947b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image='python:3.10'\n",
    ")\n",
    "def inference_svc(model_name: str, namespace: str) -> str :\n",
    "\n",
    "    inference_service = '''\n",
    "apiVersion: serving.kserve.io/v1beta1\n",
    "kind: InferenceService\n",
    "metadata:\n",
    "  name: {}\n",
    "  namespace: {}\n",
    "  annotations:\n",
    "    serving.kserve.io/deploymentMode: ModelMesh\n",
    "spec:\n",
    "  predictor:\n",
    "    model:\n",
    "      modelFormat:\n",
    "        name: peft-model\n",
    "      runtime: {}-server\n",
    "      storage:\n",
    "        key: localMinIO\n",
    "        path: sklearn/mnist-svm.joblib\n",
    "'''.format(model_name, namespace, model_name)\n",
    "\n",
    "    return inference_service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c90356a-deef-4dfa-b7dd-8b43c136edc4",
   "metadata": {},
   "source": [
    "### Test the deployed model \n",
    "\n",
    "The final pipeline step \"test_model_mesh\" executes a REST reqests against the inference service and prints out the result.\n",
    "\n",
    "If you would like to futher test the model you could execute gRPC or REST requests by ourself, but first need to port-forward service/modelmesh-serving.\n",
    "\n",
    "```bash\n",
    "kubectl port-forward --address 0.0.0.0 service/modelmesh-serving 8008 -n modelmesh-serving\n",
    "\n",
    "curl -X POST -k http://localhost:8008/v2/models/vml-demo/infer -d '{\"inputs\": [{ \"name\": \"content\", \"shape\": [1], \"datatype\": \"BYTES\", \"data\": [\"@nationalgridus I have no water and the bill is current and paid. Can you do something about this?\"]}]}' | jq -r '.outputs[0].data[0]' | base64 --decode\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97759743-4760-489e-bcac-601fa3f0d88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"transformers\", \"peft\", \"torch\", \"requests\"],\n",
    "    base_image='python:3.10'\n",
    ")\n",
    "def test_modelmesh_model(service: str,  namespace: str, model_name: str, input_tweet: str):\n",
    "    import requests\n",
    "    import base64\n",
    "    import json\n",
    "\n",
    "    url = \"http://%s.%s:8008/v2/models/%s/infer\" % (service, namespace, model_name)\n",
    "    input_json = {\n",
    "        \"inputs\": [\n",
    "            {\n",
    "            \"name\": \"content\",\n",
    "            \"shape\": [1],\n",
    "            \"datatype\": \"BYTES\",\n",
    "            \"data\": [input_tweet]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    x = requests.post(url, json = input_json)\n",
    "\n",
    "    print(x.text)\n",
    "    respond_dict = json.loads(x.text)\n",
    "    inference_result = respond_dict[\"outputs\"][0][\"data\"][0]\n",
    "    base64_bytes = inference_result.encode(\"ascii\")\n",
    "  \n",
    "    string_bytes = base64.b64decode(base64_bytes)\n",
    "    inference_result = string_bytes.decode(\"ascii\")\n",
    "    print(\"inference_result: %s \" % inference_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6cb1be-9708-487d-8174-31c5f58662b7",
   "metadata": {},
   "source": [
    "### Define your pipeline function \n",
    "#### Pipeline Input Parameters\n",
    "\n",
    "- `peft_model_server_image` - we are using pre-build custom runtime image to serve HuggingFace LLM with the prompt tuning configuration. If you would like to build your own KServe ModelMesh custom runtime image, you could do it using this [Dockerfile](https://github.com/kubeflow/kfp-tekton/blob/master/samples/peft-modelmesh-pipeline/Dockerfile)\n",
    "- `modelmesh_namespace` - namespace the modelmesh is deployed\n",
    "- `modelmesh_servicename` - modelmesh service name\n",
    "- `pipeline_out_file`- name of the yaml output file created as a result of compiling the pipeline\n",
    "- `kserv_component` - URL link to the kserve component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "869266c6-6b20-42e0-a342-64bcd9b659a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model_server_image=\"quay.io/aipipeline/peft-model-server:latest\"\n",
    "modelmesh_namespace=\"modelmesh-serving\"\n",
    "modelmesh_servicename=\"modelmesh-serving\"\n",
    "pipeline_out_file=\"llm-prompt_tuning_pipeline.yaml\"\n",
    "kserv_component=\"https://raw.githubusercontent.com/kubeflow/pipelines/release-2.0.1/components/kserve/component.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5148f7d5-3cf4-4741-ad36-32b16bb440e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your pipeline function\n",
    "@dsl.pipeline(\n",
    "    name=\"Serving LLM with Prompt tuning\",\n",
    "    description=\"A Pipeline for Serving Prompt Tuning LLMs on Modelmesh\"\n",
    ")\n",
    "def prompt_tuning_pipeline(\n",
    "    huggingface_name: str = \"difince\",\n",
    "    peft_model_publish_id: str = \"bloomz-560m_PROMPT_TUNING_CAUSAL_LM\",\n",
    "    model_name_or_path: str = \"bigscience/bloomz-560m\",\n",
    "    model_name: str = \"vml-demo\",\n",
    "    input_tweet: str = \"@nationalgridus I have no water and the bill is current and paid. Can you do something about this?\",\n",
    "    test_served_llm_model: str =\"true\",\n",
    "    num_epochs: int = 50\n",
    "):\n",
    "    hf_token_task = get_hf_token()\n",
    "    prompt_tuning_llm = prompt_tuning_bloom( huggingface_name=huggingface_name, \n",
    "                                             peft_model_publish_id=peft_model_publish_id, \n",
    "                                             model_name_or_path=model_name_or_path,\n",
    "                                             num_epochs=num_epochs,\n",
    "                                             hf_token=hf_token_task.output)\n",
    "    deploy_modelmesh_custom_runtime_task = deploy_modelmesh_custom_runtime(huggingface_name=huggingface_name,\n",
    "                                                                           peft_model_publish_id=peft_model_publish_id, \n",
    "                                                                           model_name_or_path=model_name_or_path,\n",
    "                                                                           server_name=model_name, namespace=modelmesh_namespace,\n",
    "                                                                           image=peft_model_server_image)\n",
    "    deploy_modelmesh_custom_runtime_task.after(prompt_tuning_llm)\n",
    "\n",
    "    #inference_svc just creates the Inference Service yaml and returns it as a output\n",
    "    inference_svc_task = inference_svc(model_name=model_name, namespace=modelmesh_namespace)\n",
    "    inference_svc_task.after(deploy_modelmesh_custom_runtime_task)\n",
    "    inference_svc_task.set_caching_options(False)\n",
    "    \n",
    "    # the Inference servince yaml from `inference_svc` step is passed as an input to `kserve_launcher_op` - that applies the yaml file.\n",
    "    kserve_launcher_op = comp.load_component_from_url(kserv_component)\n",
    "    serve_llm_with_peft_task = kserve_launcher_op(action=\"apply\", inferenceservice_yaml=inference_svc_task.output)\n",
    "    serve_llm_with_peft_task.after(inference_svc_task)\n",
    "    serve_llm_with_peft_task.set_caching_options(False)\n",
    "\n",
    "    with dsl.If(test_served_llm_model == 'true'):\n",
    "        test_modelmesh_model_task = test_modelmesh_model(service=modelmesh_servicename, namespace=modelmesh_namespace, \n",
    "                                                         model_name=model_name, input_tweet=input_tweet).after(serve_llm_with_peft_task)\n",
    "        test_modelmesh_model_task.set_caching_options(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163517da-d7c7-4ac4-af73-dd39ff190644",
   "metadata": {},
   "source": [
    "### Pipeline Compilation & Run \n",
    "To submit our pipeline for execution, here we have two options:\n",
    "1. To compile it to intermediate representation (IR) YAML with the KFP SDK compiler and later upload it and run it - `Compile.compile` outputs a file with name `\"llm-prompt_tuning_pipeline.yaml\"` in the current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01b5483f-9d26-4e04-ae66-852443fe7407",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp.compiler.Compiler().compile(\n",
    "    pipeline_func=prompt_tuning_pipeline,\n",
    "    package_path=pipeline_out_file\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37887ff0-2b12-4fd1-9545-34f26e65cc50",
   "metadata": {},
   "source": [
    "2. To directly call `kfp.Client` to create a run from pipeline function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9053b067-a954-4bdb-a174-d6255f3edcee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/kfp/client/client.py:158: FutureWarning: This client only works with Kubeflow Pipeline v2.0.0-beta.2 and later versions.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/experiments/details/c0defdb9-34bd-4987-bc42-54ed6cae0081\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/93404fc6-e286-4c77-b117-53104fee6cfe\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run ID:  93404fc6-e286-4c77-b117-53104fee6cfe\n"
     ]
    }
   ],
   "source": [
    "kfp_client=kfp.Client()\n",
    "\n",
    "run = kfp_client.create_run_from_pipeline_func(\n",
    "    prompt_tuning_pipeline,\n",
    "    arguments={}\n",
    ")\n",
    "\n",
    "run_id = run.run_id\n",
    "print(\"Run ID: \", run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e5ffd1-c116-4e57-b480-c0b3763242b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541fa5c4-4217-4d32-b057-2b10d8c8870c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
