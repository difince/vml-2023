{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a43166e-3c50-4a2d-886f-a978c65c73c4",
   "metadata": {},
   "source": [
    "# Kubeflow and Prompt Tunning\n",
    "\n",
    "This notebook demonstrates how Kubeflow could be levaraged for prompt tuning a foundational LLM and serving it. The HuggingFace's [PEFT](https://huggingface.co/docs/peft/index) library is used for prompt tunning the open sourced LLM. \n",
    "```\n",
    "ðŸ¤— PEFT, or Parameter-Efficient Fine-Tuning (PEFT), is a library for efficiently adapting pre-trained language models (PLMs) to various downstream applications without fine-tuning all the modelâ€™s parameters. PEFT methods only fine-tune a small number of (extra) model parameters, significantly decreasing computational and storage costs because fine-tuning large-scale PLMs is prohibitively costly. Recent state-of-the-art PEFT techniques achieve performance comparable to that of full fine-tuning.\n",
    "```\n",
    "This notebook create Kubeflow pipeline that has the following steps: \n",
    "- Reads user huggingface token from the already kubernetes created secret\n",
    "- Downloads an open source large language model (LLM) \n",
    "- Trains the prompt tuning configuration against the Hugging Face open source model (open source twitter_complaints prompt dataset).\n",
    "- Publishes a trained configuration to Hugging Face.\n",
    "- Runs inferencing on the Large Language Model with the new prompt tuning configuration\n",
    "- Tests the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f5fd16-9833-42b9-b062-a31b22f96d60",
   "metadata": {},
   "source": [
    "## Run through the Notebook\n",
    "\n",
    "After installing kfp, you must restart the kernel for the new version to take affect. After the restart, run through all cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c40ced32-fb30-4ad8-821b-e2c4d32ec90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kfp==2.3.0 in /opt/conda/lib/python3.8/site-packages (2.3.0)\n",
      "Requirement already satisfied: click<9,>=8.0.0 in /opt/conda/lib/python3.8/site-packages (from kfp==2.3.0) (8.1.7)\n",
      "Requirement already satisfied: docstring-parser<1,>=0.7.3 in /opt/conda/lib/python3.8/site-packages (from kfp==2.3.0) (0.15)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /opt/conda/lib/python3.8/site-packages (from kfp==2.3.0) (2.10.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.1 in /opt/conda/lib/python3.8/site-packages (from kfp==2.3.0) (2.23.3)\n",
      "Requirement already satisfied: google-cloud-storage<3,>=2.2.1 in /opt/conda/lib/python3.8/site-packages (from kfp==2.3.0) (2.12.0)\n",
      "Requirement already satisfied: kfp-pipeline-spec==0.2.2 in /opt/conda/lib/python3.8/site-packages (from kfp==2.3.0) (0.2.2)\n",
      "Requirement already satisfied: kfp-server-api<2.1.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from kfp==2.3.0) (2.0.2)\n",
      "Requirement already satisfied: kubernetes<27,>=8.0.0 in /opt/conda/lib/python3.8/site-packages (from kfp==2.3.0) (12.0.1)\n",
      "Requirement already satisfied: protobuf<4,>=3.13.0 in /opt/conda/lib/python3.8/site-packages (from kfp==2.3.0) (3.19.6)\n",
      "Requirement already satisfied: PyYAML<7,>=5.3 in /opt/conda/lib/python3.8/site-packages (from kfp==2.3.0) (5.4.1)\n",
      "Requirement already satisfied: requests-toolbelt<1,>=0.8.0 in /opt/conda/lib/python3.8/site-packages (from kfp==2.3.0) (0.10.1)\n",
      "Requirement already satisfied: tabulate<1,>=0.8.6 in /opt/conda/lib/python3.8/site-packages (from kfp==2.3.0) (0.9.0)\n",
      "Requirement already satisfied: urllib3<2.0.0 in /opt/conda/lib/python3.8/site-packages (from kfp==2.3.0) (1.26.16)\n",
      "Requirement already satisfied: typing-extensions<5,>=3.7.4 in /opt/conda/lib/python3.8/site-packages (from kfp==2.3.0) (4.7.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/lib/python3.8/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp==2.3.0) (1.60.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.8/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp==2.3.0) (2.31.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.1->kfp==2.3.0) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.1->kfp==2.3.0) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.1->kfp==2.3.0) (4.9)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /opt/conda/lib/python3.8/site-packages (from google-cloud-storage<3,>=2.2.1->kfp==2.3.0) (2.3.3)\n",
      "Requirement already satisfied: google-resumable-media>=2.6.0 in /opt/conda/lib/python3.8/site-packages (from google-cloud-storage<3,>=2.2.1->kfp==2.3.0) (2.6.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.8/site-packages (from google-cloud-storage<3,>=2.2.1->kfp==2.3.0) (1.5.0)\n",
      "Requirement already satisfied: six>=1.10 in /opt/conda/lib/python3.8/site-packages (from kfp-server-api<2.1.0,>=2.0.0->kfp==2.3.0) (1.16.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.8/site-packages (from kfp-server-api<2.1.0,>=2.0.0->kfp==2.3.0) (2023.7.22)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.8/site-packages (from kfp-server-api<2.1.0,>=2.0.0->kfp==2.3.0) (2.8.2)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /opt/conda/lib/python3.8/site-packages (from kubernetes<27,>=8.0.0->kfp==2.3.0) (68.1.2)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.8/site-packages (from kubernetes<27,>=8.0.0->kfp==2.3.0) (1.6.2)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.8/site-packages (from kubernetes<27,>=8.0.0->kfp==2.3.0) (1.3.1)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.1->kfp==2.3.0) (0.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.8/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp==2.3.0) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp==2.3.0) (3.4)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.8/site-packages (from requests-oauthlib->kubernetes<27,>=8.0.0->kfp==2.3.0) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install kfp==2.3.0\n",
    "# !pip install kfp \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4f783ca-3942-4884-9791-2faa4c001326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "import kfp\n",
    "\n",
    "print(kfp.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71dff4be-5c25-449e-805f-f38775222244",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "from kfp.dsl import component\n",
    "from kfp.compiler import Compiler\n",
    "import kfp.components as comp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6cb1be-9708-487d-8174-31c5f58662b7",
   "metadata": {},
   "source": [
    "### Pipeline Input Parameters\n",
    "\n",
    "- `peft_model_server_image` We are using pre-build custom runtime image to serve HuggingFace LLM with prompt tuning configuration. But here is all the code on how the HuggingFace LLM with prompt tuning configuration is being served. The _load_model function below shows we will choose a pretrained LLM model with the PEFT prompt tuning configuration trained in the prompt tuning tutorial. The tokenizer is also defined as part of the model, so it can be used to encode and decode raw string inputs from the inference requests without asking users to preprocess their input into tensor bytes.\n",
    "\n",
    "```python\n",
    "from typing import List\n",
    "\n",
    "from mlserver import MLModel, types\n",
    "from mlserver.codecs import decode_args\n",
    "\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import os\n",
    "\n",
    "class PeftModelServer(MLModel):\n",
    "  async def load(self) -> bool:\n",
    "      self._load_model()\n",
    "      self.ready = True\n",
    "      return self.ready\n",
    "\n",
    "  @decode_args\n",
    "  async def predict(self, content: List[str]) -> List[str]:\n",
    "      return self._predict_outputs(content)\n",
    "\n",
    "  def _load_model(self):\n",
    "      model_name_or_path = os.environ.get(\"PRETRAINED_MODEL_PATH\", \"bigscience/bloomz-560m\")\n",
    "      peft_model_id = os.environ.get(\"PEFT_MODEL_ID\", \"aipipeline/bloomz-560m_PROMPT_TUNING_CAUSAL_LM\")\n",
    "      self.tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "      config = PeftConfig.from_pretrained(peft_model_id)\n",
    "      self.model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path)\n",
    "      self.model = PeftModel.from_pretrained(self.model, peft_model_id)\n",
    "      self.text_column = os.environ.get(\"DATASET_TEXT_COLUMN_NAME\", \"Tweet text\")\n",
    "      return\n",
    "\n",
    "  def _predict_outputs(self, content: List[str]) -> List[str]:\n",
    "      output_list = []\n",
    "      for input in content:\n",
    "        inputs = self.tokenizer(\n",
    "            f'{self.text_column} : {input} Label : ',\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "          inputs = {k: v for k, v in inputs.items()}\n",
    "          outputs = self.model.generate(\n",
    "              input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], max_new_tokens=10, eos_token_id=3\n",
    "          )\n",
    "          outputs = self.tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)\n",
    "        output_list.append(outputs[0])\n",
    "      return output_list\n",
    "```\n",
    "If you would like to build your ouw KServe ModelMesh custom runtime image, you could do it using this Docker file:\n",
    "```\n",
    "# TODO: choose appropriate base image, install Python, MLServer, and\n",
    "# dependencies of your MLModel implementation\n",
    "FROM python:3.8-slim-buster\n",
    "RUN pip install mlserver peft transformers datasets\n",
    "# ...\n",
    "\n",
    "# The custom `MLModel` implementation should be on the Python search path\n",
    "# instead of relying on the working directory of the image. If using a\n",
    "# single-file module, this can be accomplished with:\n",
    "COPY --chown=${USER} ./peft_model_server.py /opt/peft_model_server.py\n",
    "ENV PYTHONPATH=/opt/\n",
    "\n",
    "# environment variables to be compatible with ModelMesh Serving\n",
    "# these can also be set in the ServingRuntime, but this is recommended for\n",
    "# consistency when building and testing\n",
    "ENV MLSERVER_MODELS_DIR=/models/_mlserver_models \\\n",
    "    MLSERVER_GRPC_PORT=8001 \\\n",
    "    MLSERVER_HTTP_PORT=8002 \\\n",
    "    MLSERVER_LOAD_MODELS_AT_STARTUP=false \\\n",
    "    MLSERVER_MODEL_NAME=peft-model\n",
    "\n",
    "# With this setting, the implementation field is not required in the model\n",
    "# settings which eases integration by allowing the built-in adapter to generate\n",
    "# a basic model settings file\n",
    "ENV MLSERVER_MODEL_IMPLEMENTATION=peft_model_server.PeftModelServer\n",
    "\n",
    "CMD mlserver start ${MLSERVER_MODELS_DIR}\n",
    "```\n",
    "\n",
    "\n",
    "```bash \n",
    "docker build -t <CONTAINER_REGISTRY_NAME>/peft-model-server:latest .\n",
    "docker push <CONTAINER_REGISTRY_NAME>/peft-model-server:latest\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869266c6-6b20-42e0-a342-64bcd9b659a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model_server_image=\"quay.io/aipipeline/peft-model-server:latest\"\n",
    "modelmesh_namespace=\"modelmesh-serving\"\n",
    "modelmesh_servicename=\"modelmesh-serving\"\n",
    "pipeline_out_file=\"llm-prompt_tuning_pipeline.yaml\"\n",
    "kserv_component=\"https://raw.githubusercontent.com/kubeflow/pipelines/release-2.0.1/components/kserve/component.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4ffdbf8-d014-4132-8b83-503c05125af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"peft\", \"transformers\", \"datasets\", \"torch\", \"datasets\", \"tqdm\"],\n",
    "    base_image='python:3.10'\n",
    ")\n",
    "def prompt_tuning_bloom(huggingface_name: str, peft_model_publish_id: str, model_name_or_path: str, num_epochs: int, hf_token: str):\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer, default_data_collator, get_linear_schedule_with_warmup\n",
    "    from peft import get_peft_config, get_peft_model, PromptTuningInit, PromptTuningConfig, TaskType, PeftType\n",
    "    import torch\n",
    "    from datasets import load_dataset\n",
    "    import os\n",
    "    from torch.utils.data import DataLoader\n",
    "    from tqdm import tqdm\n",
    "    import base64\n",
    "\n",
    "    peft_config = PromptTuningConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        prompt_tuning_init=PromptTuningInit.TEXT,\n",
    "        num_virtual_tokens=8,\n",
    "        prompt_tuning_init_text=\"Classify if the tweet is a complaint or not:\",\n",
    "        tokenizer_name_or_path=model_name_or_path,\n",
    "    )\n",
    "\n",
    "    dataset_name = \"twitter_complaints\"\n",
    "    text_column = \"Tweet text\"\n",
    "    label_column = \"text_label\"\n",
    "    max_length = 64\n",
    "    lr = 3e-2\n",
    "    batch_size = 8\n",
    "\n",
    "    dataset = load_dataset(\"ought/raft\", dataset_name)\n",
    "    dataset[\"train\"][0]\n",
    "\n",
    "    classes = [k.replace(\"_\", \" \") for k in dataset[\"train\"].features[\"Label\"].names]\n",
    "    dataset = dataset.map(\n",
    "        lambda x: {\"text_label\": [classes[label] for label in x[\"Label\"]]},\n",
    "        batched=True,\n",
    "        num_proc=1,\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    def preprocess_function(examples):\n",
    "        batch_size = len(examples[text_column])\n",
    "        inputs = [f\"{text_column} : {x} Label : \" for x in examples[text_column]]\n",
    "        targets = [str(x) for x in examples[label_column]]\n",
    "        model_inputs = tokenizer(inputs)\n",
    "        labels = tokenizer(targets)\n",
    "        for i in range(batch_size):\n",
    "            sample_input_ids = model_inputs[\"input_ids\"][i]\n",
    "            label_input_ids = labels[\"input_ids\"][i] + [tokenizer.pad_token_id]\n",
    "            model_inputs[\"input_ids\"][i] = sample_input_ids + label_input_ids\n",
    "            labels[\"input_ids\"][i] = [-100] * len(sample_input_ids) + label_input_ids\n",
    "            model_inputs[\"attention_mask\"][i] = [1] * len(model_inputs[\"input_ids\"][i])\n",
    "        for i in range(batch_size):\n",
    "            sample_input_ids = model_inputs[\"input_ids\"][i]\n",
    "            label_input_ids = labels[\"input_ids\"][i]\n",
    "            model_inputs[\"input_ids\"][i] = [tokenizer.pad_token_id] * (\n",
    "                max_length - len(sample_input_ids)\n",
    "            ) + sample_input_ids\n",
    "            model_inputs[\"attention_mask\"][i] = [0] * (max_length - len(sample_input_ids)) + model_inputs[\n",
    "                \"attention_mask\"\n",
    "            ][i]\n",
    "            labels[\"input_ids\"][i] = [-100] * (max_length - len(sample_input_ids)) + label_input_ids\n",
    "            model_inputs[\"input_ids\"][i] = torch.tensor(model_inputs[\"input_ids\"][i][:max_length])\n",
    "            model_inputs[\"attention_mask\"][i] = torch.tensor(model_inputs[\"attention_mask\"][i][:max_length])\n",
    "            labels[\"input_ids\"][i] = torch.tensor(labels[\"input_ids\"][i][:max_length])\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        return model_inputs\n",
    "    \n",
    "    processed_datasets = dataset.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        num_proc=1,\n",
    "        remove_columns=dataset[\"train\"].column_names,\n",
    "        load_from_cache_file=False,\n",
    "        desc=\"Running tokenizer on dataset\",\n",
    "    )\n",
    "\n",
    "    train_dataset = processed_datasets[\"train\"]\n",
    "    eval_dataset = processed_datasets[\"train\"]\n",
    "\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, shuffle=True, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=False\n",
    "    )\n",
    "    eval_dataloader = DataLoader(eval_dataset, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=False)\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    print(model.print_trainable_parameters())\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    lr_scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=(len(train_dataloader) * num_epochs),\n",
    "    )\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "            batch = {k: v for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.detach().float()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        model.eval()\n",
    "        eval_loss = 0\n",
    "        eval_preds = []\n",
    "        for step, batch in enumerate(tqdm(eval_dataloader)):\n",
    "            batch = {k: v for k, v in batch.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            eval_loss += loss.detach().float()\n",
    "            eval_preds.extend(\n",
    "                tokenizer.batch_decode(torch.argmax(outputs.logits, -1).detach().cpu().numpy(), skip_special_tokens=True)\n",
    "            )\n",
    "\n",
    "        eval_epoch_loss = eval_loss / len(eval_dataloader)\n",
    "        eval_ppl = torch.exp(eval_epoch_loss)\n",
    "        train_epoch_loss = total_loss / len(train_dataloader)\n",
    "        train_ppl = torch.exp(train_epoch_loss)\n",
    "        print(\"epoch=%s: train_ppl=%s train_epoch_loss=%s eval_ppl=%s eval_epoch_loss=%s\" % (epoch, train_ppl, train_epoch_loss, eval_ppl, eval_epoch_loss))\n",
    "\n",
    "    from huggingface_hub import login\n",
    "    login(token=base64.b64decode(hf_token).decode())\n",
    "\n",
    "    peft_model_id = \"{}/{}\".format(huggingface_name, peft_model_publish_id)\n",
    "    model.save_pretrained(\"output_dir\") \n",
    "    model.push_to_hub(peft_model_id, use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ecfb65cc-2841-468c-80a3-18cc9f7f257f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"kubernetes\"],\n",
    "    base_image='python:3.10'\n",
    ")\n",
    "def deploy_modelmesh_custom_runtime(huggingface_name: str, peft_model_publish_id: str, model_name_or_path: str, server_name: str, namespace: str, image: str):\n",
    "    import kubernetes.config as k8s_config\n",
    "    import kubernetes.client as k8s_client\n",
    "    from kubernetes.client.exceptions import ApiException\n",
    "\n",
    "    def create_custom_object(group, version, namespace, plural, manifest):\n",
    "        cfg = k8s_client.Configuration()\n",
    "        cfg.verify_ssl=False\n",
    "        cfg.host = \"https://kubernetes.default.svc\"\n",
    "        cfg.api_key_prefix['authorization'] = 'Bearer'\n",
    "        cfg.ssl_ca_cert = '/var/run/secrets/kubernetes.io/serviceaccount/ca.crt'\n",
    "        with open(\"/var/run/secrets/kubernetes.io/serviceaccount/token\") as f:\n",
    "            lines = f.readlines()\n",
    "            for l in lines:\n",
    "                cfg.api_key['authorization'] = \"{}\".format(l)\n",
    "                break\n",
    "        with k8s_client.ApiClient(cfg) as api_client:\n",
    "            capi = k8s_client.CustomObjectsApi(api_client)\n",
    "            try:\n",
    "                res = capi.create_namespaced_custom_object(group=group,\n",
    "                                                           version=version, namespace=namespace,\n",
    "                                                           plural=plural, body=manifest)\n",
    "            except ApiException as e:\n",
    "                # object already exists\n",
    "                if e.status != 409:\n",
    "                    raise\n",
    "    custom_runtime_manifest = {\n",
    "        \"apiVersion\": \"serving.kserve.io/v1alpha1\",\n",
    "        \"kind\": \"ServingRuntime\",\n",
    "        \"metadata\": {\n",
    "            \"name\": \"{}-server\".format(server_name),\n",
    "            \"namespace\": namespace\n",
    "        },\n",
    "        \"spec\": {\n",
    "            \"supportedModelFormats\": [\n",
    "            {\n",
    "                \"name\": \"peft-model\",\n",
    "                \"version\": \"1\",\n",
    "                \"autoSelect\": True\n",
    "            }\n",
    "            ],\n",
    "            \"multiModel\": True,\n",
    "            \"grpcDataEndpoint\": \"port:8001\",\n",
    "            \"grpcEndpoint\": \"port:8085\",\n",
    "            \"containers\": [\n",
    "            {\n",
    "                \"name\": \"mlserver\",\n",
    "                \"image\": image,\n",
    "                \"env\": [\n",
    "                {\n",
    "                    \"name\": \"MLSERVER_MODELS_DIR\",\n",
    "                    \"value\": \"/models/_mlserver_models/\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"MLSERVER_GRPC_PORT\",\n",
    "                    \"value\": \"8001\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"MLSERVER_HTTP_PORT\",\n",
    "                    \"value\": \"8002\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"MLSERVER_LOAD_MODELS_AT_STARTUP\",\n",
    "                    \"value\": \"true\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"MLSERVER_MODEL_NAME\",\n",
    "                    \"value\": \"peft-model\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"MLSERVER_HOST\",\n",
    "                    \"value\": \"127.0.0.1\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"MLSERVER_GRPC_MAX_MESSAGE_LENGTH\",\n",
    "                    \"value\": \"-1\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"PRETRAINED_MODEL_PATH\",\n",
    "                    \"value\": model_name_or_path\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"PEFT_MODEL_ID\",\n",
    "                    \"value\": \"{}/{}\".format(huggingface_name, peft_model_publish_id),\n",
    "                }\n",
    "                ],\n",
    "                \"resources\": {\n",
    "                \"requests\": {\n",
    "                    \"cpu\": \"500m\",\n",
    "                    \"memory\": \"4Gi\"\n",
    "                },\n",
    "                \"limits\": {\n",
    "                    \"cpu\": \"5\",\n",
    "                    \"memory\": \"5Gi\"\n",
    "                }\n",
    "                }\n",
    "            }\n",
    "            ],\n",
    "            \"builtInAdapter\": {\n",
    "            \"serverType\": \"mlserver\",\n",
    "            \"runtimeManagementPort\": 8001,\n",
    "            \"memBufferBytes\": 134217728,\n",
    "            \"modelLoadingTimeoutMillis\": 90000\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    create_custom_object(group=\"serving.kserve.io\", version=\"v1alpha1\",\n",
    "                         namespace=namespace, plural=\"servingruntimes\",\n",
    "                         manifest=custom_runtime_manifest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a2e273e4-270c-40a9-bd0b-0061983947b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image='python:3.10'\n",
    ")\n",
    "def inference_svc(model_name: str, namespace: str) -> str :\n",
    "\n",
    "    inference_service = '''\n",
    "apiVersion: serving.kserve.io/v1beta1\n",
    "kind: InferenceService\n",
    "metadata:\n",
    "  name: {}\n",
    "  namespace: {}\n",
    "  annotations:\n",
    "    serving.kserve.io/deploymentMode: ModelMesh\n",
    "spec:\n",
    "  predictor:\n",
    "    model:\n",
    "      modelFormat:\n",
    "        name: peft-model\n",
    "      runtime: {}-server\n",
    "      storage:\n",
    "        key: localMinIO\n",
    "        path: sklearn/mnist-svm.joblib\n",
    "'''.format(model_name, namespace, model_name)\n",
    "\n",
    "    return inference_service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "97759743-4760-489e-bcac-601fa3f0d88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"transformers\", \"peft\", \"torch\", \"requests\"],\n",
    "    base_image='python:3.10'\n",
    ")\n",
    "def test_modelmesh_model(service: str,  namespace: str, model_name: str, input_tweet: str):\n",
    "    import requests\n",
    "    import base64\n",
    "    import json\n",
    "\n",
    "    url = \"http://%s.%s:8008/v2/models/%s/infer\" % (service, namespace, model_name)\n",
    "    input_json = {\n",
    "        \"inputs\": [\n",
    "            {\n",
    "            \"name\": \"content\",\n",
    "            \"shape\": [1],\n",
    "            \"datatype\": \"BYTES\",\n",
    "            \"data\": [input_tweet]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    x = requests.post(url, json = input_json)\n",
    "\n",
    "    print(x.text)\n",
    "    respond_dict = json.loads(x.text)\n",
    "    inference_result = respond_dict[\"outputs\"][0][\"data\"][0]\n",
    "    base64_bytes = inference_result.encode(\"ascii\")\n",
    "  \n",
    "    string_bytes = base64.b64decode(base64_bytes)\n",
    "    inference_result = string_bytes.decode(\"ascii\")\n",
    "    print(\"inference_result: %s \" % inference_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5c724f-10e9-4bc0-b2ce-8aba440c76fb",
   "metadata": {},
   "source": [
    "### Read huggingface-secret \n",
    "\n",
    "Read the secret that contains your WRITE token for Huggingface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "43365c0f-524e-407c-ac4d-2e757ec8ceb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"kubernetes\"],\n",
    "    base_image='python:3.10'\n",
    ")\n",
    "def get_hf_token() -> str:\n",
    "    from kubernetes import client, config\n",
    "\n",
    "    config.load_incluster_config()\n",
    "    core_api = client.CoreV1Api()\n",
    "    secret = core_api.read_namespaced_secret(name=\"huggingface-secret\", namespace=\"kubeflow-user-example-com\")\n",
    "    return secret.data[\"token\"]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "46e9e8e0-8d20-498f-99a4-a76d6eaa1ed2",
   "metadata": {},
   "source": [
    "### Define your pipeline function \n",
    "\n",
    "Input pipeline parameters are: \n",
    "- `huggingface_name`: your huggingface username\n",
    "- `peft_model_publish_id`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5148f7d5-3cf4-4741-ad36-32b16bb440e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your pipeline function\n",
    "@dsl.pipeline(\n",
    "    name=\"Serving LLM with Prompt tuning\",\n",
    "    description=\"A Pipeline for Serving Prompt Tuning LLMs on Modelmesh\"\n",
    ")\n",
    "def prompt_tuning_pipeline(\n",
    "    huggingface_name: str = \"difince\",\n",
    "    peft_model_publish_id: str = \"bloomz-560m_PROMPT_TUNING_CAUSAL_LM\",\n",
    "    model_name_or_path: str = \"bigscience/bloomz-560m\",\n",
    "    model_name: str = \"vml-demo\",\n",
    "    input_tweet: str = \"@nationalgridus I have no water and the bill is current and paid. Can you do something about this?\",\n",
    "    test_served_llm_model: str =\"true\",\n",
    "    num_epochs: int = 50\n",
    "):\n",
    "    hf_token_task = get_hf_token()\n",
    "    prompt_tuning_llm = prompt_tuning_bloom( huggingface_name=huggingface_name, \n",
    "                                             peft_model_publish_id=peft_model_publish_id, \n",
    "                                             model_name_or_path=model_name_or_path,\n",
    "                                             num_epochs=num_epochs,\n",
    "                                             hf_token=hf_token_task.output)\n",
    "    deploy_modelmesh_custom_runtime_task = deploy_modelmesh_custom_runtime(huggingface_name=huggingface_name,\n",
    "                                                                           peft_model_publish_id=peft_model_publish_id, \n",
    "                                                                           model_name_or_path=model_name_or_path,\n",
    "                                                                           server_name=model_name, namespace=modelmesh_namespace,\n",
    "                                                                           image=peft_model_server_image)\n",
    "    deploy_modelmesh_custom_runtime_task.after(prompt_tuning_llm)\n",
    "\n",
    "    inference_svc_task = inference_svc(model_name=model_name, namespace=modelmesh_namespace)\n",
    "    inference_svc_task.after(deploy_modelmesh_custom_runtime_task)\n",
    "    inference_svc_task.set_caching_options(False)\n",
    "    \n",
    "    kserve_launcher_op = comp.load_component_from_url(kserv_component)\n",
    "    serve_llm_with_peft_task = kserve_launcher_op(action=\"apply\", inferenceservice_yaml=inference_svc_task.output)\n",
    "    serve_llm_with_peft_task.after(inference_svc_task)\n",
    "    serve_llm_with_peft_task.set_caching_options(False)\n",
    "\n",
    "    with dsl.If(test_served_llm_model == 'true'):\n",
    "        test_modelmesh_model_task = test_modelmesh_model(service=modelmesh_servicename, namespace=modelmesh_namespace, \n",
    "                                                         model_name=model_name, input_tweet=input_tweet).after(serve_llm_with_peft_task)\n",
    "        test_modelmesh_model_task.set_caching_options(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9053b067-a954-4bdb-a174-d6255f3edcee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/experiments/details/c0defdb9-34bd-4987-bc42-54ed6cae0081\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/255c0c06-7420-4419-8b54-469c3a1a0a4b\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run ID:  255c0c06-7420-4419-8b54-469c3a1a0a4b\n"
     ]
    }
   ],
   "source": [
    "Compiler().compile(\n",
    "    pipeline_func=prompt_tuning_pipeline,\n",
    "    package_path=\"prompt_tuning_pipeline.yaml\"\n",
    ")\n",
    "\n",
    "kfp_client=kfp.Client()\n",
    "\n",
    "run = kfp_client.create_run_from_pipeline_func(\n",
    "    prompt_tuning_pipeline,\n",
    "    arguments={}\n",
    ")\n",
    "\n",
    "run_id = run.run_id\n",
    "print(\"Run ID: \", run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b5ad23-2688-4937-a89f-91a0753b47f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
