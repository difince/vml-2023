{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a43166e-3c50-4a2d-886f-a978c65c73c4",
   "metadata": {},
   "source": [
    "# Kubeflow and Prompt Tunning\n",
    "\n",
    "This notebook demonstrates how Kubeflow could be levaraged for prompt tuning a foundational LLM and serving it. \n",
    "\n",
    "#### What's used?\n",
    "\n",
    "1. [bigscience/bloomz-560m](https://huggingface.co/bigscience/bloomz-560m#model-summary) - a foundational model prompt tunned in this notebook\n",
    "2. [PEFT](https://huggingface.co/docs/peft/index) - Parameter-Efficient Fine-Tuning (PEFT)- a ðŸ¤— HuggingFace liberary used for tine tuning/prompt tuning opensource LLM \n",
    "3. [RAFT](https://huggingface.co/datasets/ought/raft) dataset - Real-world Annotated Few-shot Tasks (RAFT) dataset - an aggregation of English-language datasets found in the real world. We are using `twitter compains` subset specified in classifing sentiment of tweets (`compain` or `not complain`)  \n",
    "\n",
    "#### The Goal ? \n",
    "<!-- Apply prompt tuning to train a bloomz-560m model on the twitter_complaints subset of the RAFT dataset. -->\n",
    "Train and update a smaller set of prompt parameters to improve the performance of a frozen-weights pretrained model in a specific downstream task instead of fully finetuning a separate model.\n",
    "In our specific case:\n",
    "\n",
    "1. Improve `bigscience/bloomz-560m`'s ability to determine tweets' sentiment by prompt tuning it. \n",
    "2. Automate the prompt tuning and serving processes via Kubeflow Pipelines.\n",
    "\n",
    "#### Helpfull links\n",
    "- [vml-2023](https://github.com/difince/vml-2023) - a github reporitory which contains the code of this notebook and setup instructions. \n",
    "- Other:\n",
    "    - [PEFT-prompt-tuning](https://huggingface.co/docs/peft/task_guides/clm-prompt-tuning)\n",
    "<!-- \n",
    "This notebook create Kubeflow pipeline that has the following steps: \n",
    "- Reads user huggingface token from the already kubernetes created secret\n",
    "- Downloads an open source large language model (LLM) \n",
    "- Trains the prompt tuning configuration against the Hugging Face open source model (open source twitter_complaints prompt dataset).\n",
    "- Publishes a trained configuration to Hugging Face.\n",
    "- Runs inferencing on the Large Language Model with the new prompt tuning configuration\n",
    "- Tests the model -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f5fd16-9833-42b9-b062-a31b22f96d60",
   "metadata": {},
   "source": [
    "### Run through the Notebook\n",
    "\n",
    "After installing kfp, you must restart the kernel for the new version to take affect. After the restart, run through all cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c40ced32-fb30-4ad8-821b-e2c4d32ec90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kfp==2.3.0\n",
      "  Using cached kfp-2.3.0-py3-none-any.whl\n",
      "Collecting click<9,>=8.0.0 (from kfp==2.3.0)\n",
      "  Obtaining dependency information for click<9,>=8.0.0 from https://files.pythonhosted.org/packages/00/2e/d53fa4befbf2cfa713304affc7ca780ce4fc1fd8710527771b58311a3229/click-8.1.7-py3-none-any.whl.metadata\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: docstring-parser<1,>=0.7.3 in /opt/conda/lib/python3.8/site-packages (from kfp==2.3.0) (0.15)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /opt/conda/lib/python3.8/site-packages (from kfp==2.3.0) (2.10.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.1 in /opt/conda/lib/python3.8/site-packages (from kfp==2.3.0) (1.35.0)\n",
      "Collecting google-cloud-storage<3,>=2.2.1 (from kfp==2.3.0)\n",
      "  Obtaining dependency information for google-cloud-storage<3,>=2.2.1 from https://files.pythonhosted.org/packages/04/72/71b1b531cefa1daff8f6a2a70b4d4fa18dd4da851b5486d53578811b0838/google_cloud_storage-2.13.0-py2.py3-none-any.whl.metadata\n",
      "  Using cached google_cloud_storage-2.13.0-py2.py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting kfp-pipeline-spec==0.2.2 (from kfp==2.3.0)\n",
      "  Using cached kfp_pipeline_spec-0.2.2-py3-none-any.whl (20 kB)\n",
      "Collecting kfp-server-api<2.1.0,>=2.0.0 (from kfp==2.3.0)\n",
      "  Using cached kfp_server_api-2.0.3-py3-none-any.whl\n",
      "Requirement already satisfied: kubernetes<27,>=8.0.0 in /opt/conda/lib/python3.8/site-packages (from kfp==2.3.0) (12.0.1)\n",
      "Requirement already satisfied: protobuf<4,>=3.13.0 in /opt/conda/lib/python3.8/site-packages (from kfp==2.3.0) (3.19.6)\n",
      "Requirement already satisfied: PyYAML<7,>=5.3 in /opt/conda/lib/python3.8/site-packages (from kfp==2.3.0) (5.4.1)\n",
      "Requirement already satisfied: requests-toolbelt<1,>=0.8.0 in /opt/conda/lib/python3.8/site-packages (from kfp==2.3.0) (0.10.1)\n",
      "Requirement already satisfied: tabulate<1,>=0.8.6 in /opt/conda/lib/python3.8/site-packages (from kfp==2.3.0) (0.9.0)\n",
      "Requirement already satisfied: urllib3<2.0.0 in /opt/conda/lib/python3.8/site-packages (from kfp==2.3.0) (1.26.16)\n",
      "Requirement already satisfied: typing-extensions<5,>=3.7.4 in /opt/conda/lib/python3.8/site-packages (from kfp==2.3.0) (4.7.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/lib/python3.8/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp==2.3.0) (1.60.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.8/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp==2.3.0) (2.31.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.1->kfp==2.3.0) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.1->kfp==2.3.0) (0.3.0)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.1->kfp==2.3.0) (68.1.2)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.1->kfp==2.3.0) (1.16.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.1->kfp==2.3.0) (4.9)\n",
      "Collecting google-auth<3,>=1.6.1 (from kfp==2.3.0)\n",
      "  Obtaining dependency information for google-auth<3,>=1.6.1 from https://files.pythonhosted.org/packages/86/a7/75911c13a242735d5aeaca6a272da380335ff4ba5f26d6b2ae20ff682d13/google_auth-2.23.4-py2.py3-none-any.whl.metadata\n",
      "  Using cached google_auth-2.23.4-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /opt/conda/lib/python3.8/site-packages (from google-cloud-storage<3,>=2.2.1->kfp==2.3.0) (2.3.3)\n",
      "Requirement already satisfied: google-resumable-media>=2.6.0 in /opt/conda/lib/python3.8/site-packages (from google-cloud-storage<3,>=2.2.1->kfp==2.3.0) (2.6.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.8/site-packages (from google-cloud-storage<3,>=2.2.1->kfp==2.3.0) (1.5.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.8/site-packages (from kfp-server-api<2.1.0,>=2.0.0->kfp==2.3.0) (2023.7.22)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.8/site-packages (from kfp-server-api<2.1.0,>=2.0.0->kfp==2.3.0) (2.8.2)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.8/site-packages (from kubernetes<27,>=8.0.0->kfp==2.3.0) (1.6.2)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.8/site-packages (from kubernetes<27,>=8.0.0->kfp==2.3.0) (1.3.1)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.1->kfp==2.3.0) (0.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.8/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp==2.3.0) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp==2.3.0) (3.4)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.8/site-packages (from requests-oauthlib->kubernetes<27,>=8.0.0->kfp==2.3.0) (3.2.2)\n",
      "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Using cached google_cloud_storage-2.13.0-py2.py3-none-any.whl (121 kB)\n",
      "Using cached google_auth-2.23.4-py2.py3-none-any.whl (183 kB)\n",
      "Installing collected packages: kfp-pipeline-spec, click, kfp-server-api, google-auth, google-cloud-storage, kfp\n",
      "  Attempting uninstall: kfp-pipeline-spec\n",
      "    Found existing installation: kfp-pipeline-spec 0.1.16\n",
      "    Uninstalling kfp-pipeline-spec-0.1.16:\n",
      "      Successfully uninstalled kfp-pipeline-spec-0.1.16\n",
      "  Attempting uninstall: click\n",
      "    Found existing installation: click 7.1.2\n",
      "    Uninstalling click-7.1.2:\n",
      "      Successfully uninstalled click-7.1.2\n",
      "  Attempting uninstall: kfp-server-api\n",
      "    Found existing installation: kfp-server-api 1.6.0\n",
      "    Uninstalling kfp-server-api-1.6.0:\n",
      "      Successfully uninstalled kfp-server-api-1.6.0\n",
      "  Attempting uninstall: google-auth\n",
      "    Found existing installation: google-auth 1.35.0\n",
      "    Uninstalling google-auth-1.35.0:\n",
      "      Successfully uninstalled google-auth-1.35.0\n",
      "  Attempting uninstall: google-cloud-storage\n",
      "    Found existing installation: google-cloud-storage 1.44.0\n",
      "    Uninstalling google-cloud-storage-1.44.0:\n",
      "      Successfully uninstalled google-cloud-storage-1.44.0\n",
      "  Attempting uninstall: kfp\n",
      "    Found existing installation: kfp 1.6.3\n",
      "    Uninstalling kfp-1.6.3:\n",
      "      Successfully uninstalled kfp-1.6.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.9.3 requires absl-py>=1.0.0, but you have absl-py 0.11.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed click-8.1.7 google-auth-2.23.4 google-cloud-storage-2.13.0 kfp-2.3.0 kfp-pipeline-spec-0.2.2 kfp-server-api-2.0.3\n"
     ]
    }
   ],
   "source": [
    "!pip install kfp==2.3.0\n",
    "# !pip install kfp \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4f783ca-3942-4884-9791-2faa4c001326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "from kfp.dsl import component\n",
    "from kfp.compiler import Compiler\n",
    "import kfp.components as comp\n",
    "\n",
    "print(kfp.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84402db0-d5e9-405c-9fcc-3087e7338f9e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Kubeflow Pipeline\n",
    "A Kubeflow pipeline (KFP) is used for building and deploying portable and scalable machine learning (ML) workflows, based on `Docker containers`. A pipeline is composed of a set of input parameters and a list of the steps in this workflow. Each step in a pipeline is an instance of a component.\n",
    "\n",
    "With KFP you can \n",
    "- author components and pipelines using the `KFP Python SDK`, \n",
    "- compile pipelines to an intermediate representation YAML, \n",
    "- submit the pipeline to KFP-conformant backend for execution.\n",
    "\n",
    "The `dsl.component` and `dsl.pipeline` decorators turn your type-annotated Python functions into components and pipelines, respectively. \n",
    "\n",
    "Each Pipeline component is executed in a separate container. For each we need to define the `packages_to_install` and the `base_image`\n",
    "<!-- The PromptTuningConfig contains information about the task type, the text to initialize the prompt embedding, the number of virtual tokens, and the tokenizer to use -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5c724f-10e9-4bc0-b2ce-8aba440c76fb",
   "metadata": {},
   "source": [
    "### Read huggingface-secret \n",
    "\n",
    "A kubernetes secret that contains User Huggingface Write token needs to be created prior runing this pipeline. The secret can be created with the following command: \n",
    "\n",
    "`kubectl create secret generic huggingface-secret --from-literal='token=<HuggingFace_WRITE_Token>' -n kubeflow`\n",
    "\n",
    "get_hf_token Pipeline component reads this secret, exctracts the HuggingFace token and pass it as an parameter to the following Pipeline steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43365c0f-524e-407c-ac4d-2e757ec8ceb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"kubernetes\"],\n",
    "    base_image='python:3.10'\n",
    ")\n",
    "def get_hf_token() -> str:\n",
    "    from kubernetes import client, config\n",
    "\n",
    "    config.load_incluster_config()\n",
    "    core_api = client.CoreV1Api()\n",
    "    secret = core_api.read_namespaced_secret(name=\"huggingface-secret\", namespace=\"kubeflow-user-example-com\")\n",
    "    return secret.data[\"token\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32314ed1-199d-4a22-bd84-e9e6384c7b8b",
   "metadata": {},
   "source": [
    "### Prompt Tuning \n",
    "\"prompt_tuning_bloom\" pipeline step defines the model and tokenizer, the dataset and the dataset columns to train on, some training hyperparameters, and the PromptTuningConfig. The PromptTuningConfig contains information about the task type, the text to initialize the prompt embedding, the number of virtual tokens, and the tokenizer to use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4ffdbf8-d014-4132-8b83-503c05125af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"peft\", \"transformers\", \"datasets\", \"torch\", \"datasets\", \"tqdm\"],\n",
    "    base_image='python:3.10'\n",
    ")\n",
    "def prompt_tuning_bloom(huggingface_name: str, peft_model_publish_id: str, model_name_or_path: str, num_epochs: int, hf_token: str):\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer, default_data_collator, get_linear_schedule_with_warmup\n",
    "    from peft import get_peft_config, get_peft_model, PromptTuningInit, PromptTuningConfig, TaskType, PeftType\n",
    "    import torch\n",
    "    from datasets import load_dataset\n",
    "    import os\n",
    "    from torch.utils.data import DataLoader\n",
    "    from tqdm import tqdm\n",
    "    import base64\n",
    "\n",
    "    peft_config = PromptTuningConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        prompt_tuning_init=PromptTuningInit.TEXT,\n",
    "        num_virtual_tokens=8,\n",
    "        prompt_tuning_init_text=\"Classify if the tweet is a complaint or not:\",\n",
    "        tokenizer_name_or_path=model_name_or_path, # bigscience/bloomz-560m\n",
    "    )\n",
    "\n",
    "    dataset_name = \"twitter_complaints\"\n",
    "    text_column = \"Tweet text\"\n",
    "    label_column = \"text_label\"\n",
    "    max_length = 64\n",
    "    lr = 3e-2\n",
    "    batch_size = 8\n",
    "\n",
    "    # Load the twitter_complaints subset of the RAFT dataset. This subset contains tweets that are labeled either complaint or no complaint\n",
    "    dataset = load_dataset(\"ought/raft\", dataset_name)\n",
    "    dataset[\"train\"][0]\n",
    "\n",
    "    # Make the Label column more readable -> replace the Label value with the corresponding label text and store them in a `text_label` column. The `map` function makes it possible to apply this change over the entire dataset in one step.\n",
    "    classes = [k.replace(\"_\", \" \") for k in dataset[\"train\"].features[\"Label\"].names]\n",
    "    dataset = dataset.map(\n",
    "        lambda x: {\"text_label\": [classes[label] for label in x[\"Label\"]]},\n",
    "        batched=True,\n",
    "        num_proc=1,\n",
    "    )\n",
    "\n",
    "    # Preprocess dataset - by setting tokenizer.\n",
    "    # Configure the appropriate padding token to use for padding sequences, and determine the maximum length of the tokenized labels\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "        \n",
    "    # Create a preprocess_function to:\n",
    "    # Tokenize the input text and labels.\n",
    "    # For each example in a batch, pad the labels with the tokenizers pad_token_id.\n",
    "    # Concatenate the input text and labels into the model_inputs.\n",
    "    # Create a separate attention mask for labels and model_inputs.\n",
    "    # Loop through each example in the batch again to pad the input ids, labels, and attention mask to the max_length and convert them to PyTorch tensors.\n",
    "    def preprocess_function(examples):\n",
    "        batch_size = len(examples[text_column])\n",
    "        inputs = [f\"{text_column} : {x} Label : \" for x in examples[text_column]]\n",
    "        targets = [str(x) for x in examples[label_column]]\n",
    "        model_inputs = tokenizer(inputs)\n",
    "        labels = tokenizer(targets)\n",
    "        for i in range(batch_size):\n",
    "            sample_input_ids = model_inputs[\"input_ids\"][i]\n",
    "            label_input_ids = labels[\"input_ids\"][i] + [tokenizer.pad_token_id]\n",
    "            model_inputs[\"input_ids\"][i] = sample_input_ids + label_input_ids\n",
    "            labels[\"input_ids\"][i] = [-100] * len(sample_input_ids) + label_input_ids\n",
    "            model_inputs[\"attention_mask\"][i] = [1] * len(model_inputs[\"input_ids\"][i])\n",
    "        for i in range(batch_size):\n",
    "            sample_input_ids = model_inputs[\"input_ids\"][i]\n",
    "            label_input_ids = labels[\"input_ids\"][i]\n",
    "            model_inputs[\"input_ids\"][i] = [tokenizer.pad_token_id] * (\n",
    "                max_length - len(sample_input_ids)\n",
    "            ) + sample_input_ids\n",
    "            model_inputs[\"attention_mask\"][i] = [0] * (max_length - len(sample_input_ids)) + model_inputs[\n",
    "                \"attention_mask\"\n",
    "            ][i]\n",
    "            labels[\"input_ids\"][i] = [-100] * (max_length - len(sample_input_ids)) + label_input_ids\n",
    "            model_inputs[\"input_ids\"][i] = torch.tensor(model_inputs[\"input_ids\"][i][:max_length])\n",
    "            model_inputs[\"attention_mask\"][i] = torch.tensor(model_inputs[\"attention_mask\"][i][:max_length])\n",
    "            labels[\"input_ids\"][i] = torch.tensor(labels[\"input_ids\"][i][:max_length])\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        return model_inputs\n",
    "    \n",
    "    # Use the map function to apply the preprocess_function to the entire dataset. You can remove the unprocessed columns since the model wonâ€™t need them:\n",
    "    processed_datasets = dataset.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        num_proc=1,\n",
    "        remove_columns=dataset[\"train\"].column_names,\n",
    "        load_from_cache_file=False,\n",
    "        desc=\"Running tokenizer on dataset\",\n",
    "    )\n",
    "\n",
    "    # Create a DataLoader from the train and eval datasets. Set pin_memory=True to speed up the data transfer to the GPU during training if the samples in your dataset are on a CPU.\n",
    "    train_dataset = processed_datasets[\"train\"]\n",
    "    eval_dataset = processed_datasets[\"test\"]\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, shuffle=True, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=False\n",
    "    )\n",
    "    eval_dataloader = DataLoader(eval_dataset, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=False)\n",
    "    \n",
    "    # Initialize a base model from AutoModelForCausalLM, and pass it and peft_config to the get_peft_model() function to create a PeftModel. \n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name_or_path) #  bigscience/bloomz-560m\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    \n",
    "    # Print the new PeftModelâ€™s trainable parameters to see how much more efficient it is than training the full parameters of the original model!\n",
    "    print(model.print_trainable_parameters())\n",
    "    \n",
    "    # Setup an optimizer and learning rate scheduler:\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    lr_scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=(len(train_dataloader) * num_epochs),\n",
    "    )\n",
    "\n",
    "    # Define training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "            batch = {k: v for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.detach().float()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        model.eval()\n",
    "        eval_loss = 0\n",
    "        eval_preds = []\n",
    "        for step, batch in enumerate(tqdm(eval_dataloader)):\n",
    "            batch = {k: v for k, v in batch.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            eval_loss += loss.detach().float()\n",
    "            eval_preds.extend(\n",
    "                tokenizer.batch_decode(torch.argmax(outputs.logits, -1).detach().cpu().numpy(), skip_special_tokens=True)\n",
    "            )\n",
    "\n",
    "        eval_epoch_loss = eval_loss / len(eval_dataloader)\n",
    "        eval_ppl = torch.exp(eval_epoch_loss)\n",
    "        train_epoch_loss = total_loss / len(train_dataloader)\n",
    "        train_ppl = torch.exp(train_epoch_loss)\n",
    "        print(\"epoch=%s: train_ppl=%s train_epoch_loss=%s eval_ppl=%s eval_epoch_loss=%s\" % (epoch, train_ppl, train_epoch_loss, eval_ppl, eval_epoch_loss))\n",
    "\n",
    "    # Store and share your model \n",
    "    from huggingface_hub import login\n",
    "    login(token=base64.b64decode(hf_token).decode())\n",
    "\n",
    "    peft_model_id = \"{}/{}\".format(huggingface_name, peft_model_publish_id)\n",
    "    model.save_pretrained(\"output_dir\")\n",
    "    \n",
    "    # Use the push_to_hub function to upload your model to a model repository on the Hub:\n",
    "    model.push_to_hub(peft_model_id, use_auth_token=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefa2d00-fff9-49cf-a6d0-608b77604d5f",
   "metadata": {},
   "source": [
    "### Deploy Serving Runtime\n",
    "\n",
    "A KServe's ServingRuntimeand ClusterServingRuntime api-resources define the templates for Pods that can serve one or more particular model formats. Each ServingRuntime defines key information such as the container image of the runtime and a list of the model formats that the runtime supports. Other configuration settings for the runtime can be conveyed through environment variables in the container specification.\n",
    "\n",
    "Several out-of-the-box ClusterServingRuntimes are provided with KServe so that users can quickly deploy common model formats without having to define the runtimes themselves.\n",
    "```bash \n",
    "    kubectl get ClusterServingRuntimes\n",
    "```\n",
    "But in our example we extend the KServe installation by adding Custom Serving Runtime - see \"custom_runtime_manifest\". We use pre-created image for serving \"quay.io/aipipeline/peft-model-server:latest\", but if you would like to create it by yourself take a look at [peft_model_server.py](https://github.com/kubeflow/kfp-tekton/blob/master/samples/peft-modelmesh-pipeline/peft_model_server.py) and the [Dockerfile](https://github.com/kubeflow/kfp-tekton/blob/master/samples/peft-modelmesh-pipeline/Dockerfile) \n",
    "Once this step is executed you could validate the CSR is creted by: \n",
    "```bash \n",
    "kubectl get  servingruntime -n modelmesh-serving\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecfb65cc-2841-468c-80a3-18cc9f7f257f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"kubernetes\"],\n",
    "    base_image='python:3.10'\n",
    ")\n",
    "def deploy_modelmesh_custom_runtime(huggingface_name: str, peft_model_publish_id: str, model_name_or_path: str, server_name: str, namespace: str, image: str):\n",
    "    import kubernetes.config as k8s_config\n",
    "    import kubernetes.client as k8s_client\n",
    "    from kubernetes.client.exceptions import ApiException\n",
    "\n",
    "    def create_custom_object(group, version, namespace, plural, manifest):\n",
    "        cfg = k8s_client.Configuration()\n",
    "        cfg.verify_ssl=False\n",
    "        cfg.host = \"https://kubernetes.default.svc\"\n",
    "        cfg.api_key_prefix['authorization'] = 'Bearer'\n",
    "        cfg.ssl_ca_cert = '/var/run/secrets/kubernetes.io/serviceaccount/ca.crt'\n",
    "        with open(\"/var/run/secrets/kubernetes.io/serviceaccount/token\") as f:\n",
    "            lines = f.readlines()\n",
    "            for l in lines:\n",
    "                cfg.api_key['authorization'] = \"{}\".format(l)\n",
    "                break\n",
    "        with k8s_client.ApiClient(cfg) as api_client:\n",
    "            capi = k8s_client.CustomObjectsApi(api_client)\n",
    "            try:\n",
    "                res = capi.create_namespaced_custom_object(group=group,\n",
    "                                                           version=version, namespace=namespace,\n",
    "                                                           plural=plural, body=manifest)\n",
    "            except ApiException as e:\n",
    "                # object already exists\n",
    "                if e.status != 409:\n",
    "                    raise\n",
    "    custom_runtime_manifest = {\n",
    "        \"apiVersion\": \"serving.kserve.io/v1alpha1\",\n",
    "        \"kind\": \"ServingRuntime\",\n",
    "        \"metadata\": {\n",
    "            \"name\": \"{}-server\".format(server_name), #vml-demo-server\n",
    "            \"namespace\": namespace\n",
    "        },\n",
    "        \"spec\": {\n",
    "            \"supportedModelFormats\": [\n",
    "            {\n",
    "                \"name\": \"peft-model\",\n",
    "                \"version\": \"1\",\n",
    "                \"autoSelect\": True\n",
    "            }\n",
    "            ],\n",
    "            \"multiModel\": True,\n",
    "            \"grpcDataEndpoint\": \"port:8001\",\n",
    "            \"grpcEndpoint\": \"port:8085\",\n",
    "            \"containers\": [\n",
    "            {\n",
    "                \"name\": \"mlserver\",\n",
    "                \"image\": image, # quay.io/aipipeline/peft-model-server:latest\n",
    "                \"env\": [\n",
    "                {\n",
    "                    \"name\": \"MLSERVER_MODELS_DIR\",\n",
    "                    \"value\": \"/models/_mlserver_models/\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"MLSERVER_GRPC_PORT\",\n",
    "                    \"value\": \"8001\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"MLSERVER_HTTP_PORT\",\n",
    "                    \"value\": \"8002\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"MLSERVER_LOAD_MODELS_AT_STARTUP\",\n",
    "                    \"value\": \"true\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"MLSERVER_MODEL_NAME\",\n",
    "                    \"value\": \"peft-model\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"MLSERVER_HOST\",\n",
    "                    \"value\": \"127.0.0.1\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"MLSERVER_GRPC_MAX_MESSAGE_LENGTH\",\n",
    "                    \"value\": \"-1\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"PRETRAINED_MODEL_PATH\",\n",
    "                    \"value\": model_name_or_path\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"PEFT_MODEL_ID\",\n",
    "                    \"value\": \"{}/{}\".format(huggingface_name, peft_model_publish_id),\n",
    "                }\n",
    "                ],\n",
    "                \"resources\": {\n",
    "                \"requests\": {\n",
    "                    \"cpu\": \"500m\",\n",
    "                    \"memory\": \"4Gi\"\n",
    "                },\n",
    "                \"limits\": {\n",
    "                    \"cpu\": \"5\",\n",
    "                    \"memory\": \"5Gi\"\n",
    "                }\n",
    "                }\n",
    "            }\n",
    "            ],\n",
    "            \"builtInAdapter\": {\n",
    "            \"serverType\": \"mlserver\",\n",
    "            \"runtimeManagementPort\": 8001,\n",
    "            \"memBufferBytes\": 134217728,\n",
    "            \"modelLoadingTimeoutMillis\": 90000\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    create_custom_object(group=\"serving.kserve.io\", version=\"v1alpha1\",\n",
    "                         namespace=namespace, plural=\"servingruntimes\",\n",
    "                         manifest=custom_runtime_manifest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406adf5d-6e75-43cd-bb9d-ad2488037a71",
   "metadata": {},
   "source": [
    "### Deploy the Model\n",
    "\n",
    "Create KServe InferenceService \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2e273e4-270c-40a9-bd0b-0061983947b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image='python:3.10'\n",
    ")\n",
    "def inference_svc(model_name: str, namespace: str) -> str :\n",
    "\n",
    "    inference_service = '''\n",
    "apiVersion: serving.kserve.io/v1beta1\n",
    "kind: InferenceService\n",
    "metadata:\n",
    "  name: {}\n",
    "  namespace: {}\n",
    "  annotations:\n",
    "    serving.kserve.io/deploymentMode: ModelMesh\n",
    "spec:\n",
    "  predictor:\n",
    "    model:\n",
    "      modelFormat:\n",
    "        name: peft-model\n",
    "      runtime: {}-server\n",
    "      storage:\n",
    "        key: localMinIO\n",
    "        path: sklearn/mnist-svm.joblib\n",
    "'''.format(model_name, namespace, model_name)\n",
    "\n",
    "    return inference_service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c90356a-deef-4dfa-b7dd-8b43c136edc4",
   "metadata": {},
   "source": [
    "### Test the deployed model \n",
    "\n",
    "The final pipeline step \"test_model_mesh\" executes a REST reqests against the inference service and prints out the result.\n",
    "\n",
    "If you would like to futher test the model you could execute gRPC or REST requests by ourself, but first need to port-forward service/modelmesh-serving.\n",
    "\n",
    "```bash\n",
    "kubectl port-forward --address 0.0.0.0 service/modelmesh-serving 8008 -n modelmesh-serving\n",
    "\n",
    "curl -X POST -k http://localhost:8008/v2/models/vml-demo/infer -d '{\"inputs\": [{ \"name\": \"content\", \"shape\": [1], \"datatype\": \"BYTES\", \"data\": [\"@nationalgridus I have no water and the bill is current and paid. Can you do something about this?\"]}]}' | jq -r '.outputs[0].data[0]' | base64 --decode\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97759743-4760-489e-bcac-601fa3f0d88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"transformers\", \"peft\", \"torch\", \"requests\"],\n",
    "    base_image='python:3.10'\n",
    ")\n",
    "def test_modelmesh_model(service: str,  namespace: str, model_name: str, input_tweet: str):\n",
    "    import requests\n",
    "    import base64\n",
    "    import json\n",
    "\n",
    "    url = \"http://%s.%s:8008/v2/models/%s/infer\" % (service, namespace, model_name)\n",
    "    input_json = {\n",
    "        \"inputs\": [\n",
    "            {\n",
    "            \"name\": \"content\",\n",
    "            \"shape\": [1],\n",
    "            \"datatype\": \"BYTES\",\n",
    "            \"data\": [input_tweet]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    x = requests.post(url, json = input_json)\n",
    "\n",
    "    print(x.text)\n",
    "    respond_dict = json.loads(x.text)\n",
    "    inference_result = respond_dict[\"outputs\"][0][\"data\"][0]\n",
    "    base64_bytes = inference_result.encode(\"ascii\")\n",
    "  \n",
    "    string_bytes = base64.b64decode(base64_bytes)\n",
    "    inference_result = string_bytes.decode(\"ascii\")\n",
    "    print(\"inference_result: %s \" % inference_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6cb1be-9708-487d-8174-31c5f58662b7",
   "metadata": {},
   "source": [
    "### Define your pipeline function \n",
    "#### Pipeline Input Parameters\n",
    "\n",
    "- `peft_model_server_image` - we are using pre-build custom runtime image to serve HuggingFace LLM with the prompt tuning configuration. If you would like to build your own KServe ModelMesh custom runtime image, you could do it using this [Dockerfile](https://github.com/kubeflow/kfp-tekton/blob/master/samples/peft-modelmesh-pipeline/Dockerfile)\n",
    "- `modelmesh_namespace` - namespace the modelmesh is deployed\n",
    "- `modelmesh_servicename` - \n",
    "- `pipeline_out_file`- name of the yaml output file created as a result of compiling the pipeline\n",
    "- `kserv_component` - URL link to the kserve component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "869266c6-6b20-42e0-a342-64bcd9b659a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model_server_image=\"quay.io/aipipeline/peft-model-server:latest\"\n",
    "modelmesh_namespace=\"modelmesh-serving\"\n",
    "modelmesh_servicename=\"modelmesh-serving\"\n",
    "pipeline_out_file=\"llm-prompt_tuning_pipeline.yaml\"\n",
    "kserv_component=\"https://raw.githubusercontent.com/kubeflow/pipelines/release-2.0.1/components/kserve/component.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5148f7d5-3cf4-4741-ad36-32b16bb440e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your pipeline function\n",
    "@dsl.pipeline(\n",
    "    name=\"Serving LLM with Prompt tuning\",\n",
    "    description=\"A Pipeline for Serving Prompt Tuning LLMs on Modelmesh\"\n",
    ")\n",
    "def prompt_tuning_pipeline(\n",
    "    huggingface_name: str = \"difince\",\n",
    "    peft_model_publish_id: str = \"bloomz-560m_PROMPT_TUNING_CAUSAL_LM\",\n",
    "    model_name_or_path: str = \"bigscience/bloomz-560m\",\n",
    "    model_name: str = \"vml-demo\",\n",
    "    input_tweet: str = \"@nationalgridus I have no water and the bill is current and paid. Can you do something about this?\",\n",
    "    test_served_llm_model: str =\"true\",\n",
    "    num_epochs: int = 50\n",
    "):\n",
    "    hf_token_task = get_hf_token()\n",
    "    prompt_tuning_llm = prompt_tuning_bloom( huggingface_name=huggingface_name, \n",
    "                                             peft_model_publish_id=peft_model_publish_id, \n",
    "                                             model_name_or_path=model_name_or_path,\n",
    "                                             num_epochs=num_epochs,\n",
    "                                             hf_token=hf_token_task.output)\n",
    "    deploy_modelmesh_custom_runtime_task = deploy_modelmesh_custom_runtime(huggingface_name=huggingface_name,\n",
    "                                                                           peft_model_publish_id=peft_model_publish_id, \n",
    "                                                                           model_name_or_path=model_name_or_path,\n",
    "                                                                           server_name=model_name, namespace=modelmesh_namespace,\n",
    "                                                                           image=peft_model_server_image)\n",
    "    deploy_modelmesh_custom_runtime_task.after(prompt_tuning_llm)\n",
    "\n",
    "    inference_svc_task = inference_svc(model_name=model_name, namespace=modelmesh_namespace)\n",
    "    inference_svc_task.after(deploy_modelmesh_custom_runtime_task)\n",
    "    inference_svc_task.set_caching_options(False)\n",
    "    \n",
    "    kserve_launcher_op = comp.load_component_from_url(kserv_component)\n",
    "    serve_llm_with_peft_task = kserve_launcher_op(action=\"apply\", inferenceservice_yaml=inference_svc_task.output)\n",
    "    serve_llm_with_peft_task.after(inference_svc_task)\n",
    "    serve_llm_with_peft_task.set_caching_options(False)\n",
    "\n",
    "    with dsl.If(test_served_llm_model == 'true'):\n",
    "        test_modelmesh_model_task = test_modelmesh_model(service=modelmesh_servicename, namespace=modelmesh_namespace, \n",
    "                                                         model_name=model_name, input_tweet=input_tweet).after(serve_llm_with_peft_task)\n",
    "        test_modelmesh_model_task.set_caching_options(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163517da-d7c7-4ac4-af73-dd39ff190644",
   "metadata": {},
   "source": [
    "### Pipeline Compilation & Run \n",
    "To submit our pipeline for execution, here we have two options:\n",
    "1. To compile it to intermediate representation (IR) YAML with the KFP SDK compiler and later upload it and run it - `Compile.compile` outputs a file with name `\"llm-prompt_tuning_pipeline.yaml\"` in the current directory.\n",
    "2. To directly call `kfp.Client` to create a run from pipeline function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9053b067-a954-4bdb-a174-d6255f3edcee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/kfp/client/client.py:158: FutureWarning: This client only works with Kubeflow Pipeline v2.0.0-beta.2 and later versions.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/experiments/details/c0defdb9-34bd-4987-bc42-54ed6cae0081\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/495dd212-105a-425a-bc8b-051841f810ea\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run ID:  495dd212-105a-425a-bc8b-051841f810ea\n"
     ]
    }
   ],
   "source": [
    "Compiler().compile(\n",
    "    pipeline_func=prompt_tuning_pipeline,\n",
    "    package_path=pipeline_out_file\n",
    ")\n",
    "\n",
    "kfp_client=kfp.Client()\n",
    "\n",
    "run = kfp_client.create_run_from_pipeline_func(\n",
    "    prompt_tuning_pipeline,\n",
    "    arguments={}\n",
    ")\n",
    "\n",
    "run_id = run.run_id\n",
    "print(\"Run ID: \", run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b5ad23-2688-4937-a89f-91a0753b47f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
